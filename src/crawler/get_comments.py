import asyncio
import json
import csv
import os
import base64
import re
from datetime import datetime
from playwright.async_api import async_playwright

# ==============================================================================
# C·∫§U H√åNH
# ==============================================================================
INPUT_POSTS_FILE = 'data/crawler/posts_detail.csv'          # File ƒë·∫ßu v√†o
OUTPUT_COMMENTS_FILE = 'data/crawler/comments_detail.csv'   # File ƒë·∫ßu ra
CURRENT_PROFILE_NAME = "acc_clone_1"

SCROLL_DELAY = 3      # Th·ªùi gian ngh·ªâ khi cu·ªôn
MAX_RETRIES = 3       # S·ªë l·∫ßn th·ª≠ cu·ªôn l·∫°i n·∫øu h·∫øt comment

class FacebookCommentCrawler:
    def __init__(self):
        """Kh·ªüi t·∫°o Class"""
        self.input_path = os.path.join(os.getcwd(), INPUT_POSTS_FILE)
        self.output_path = os.path.join(os.getcwd(), OUTPUT_COMMENTS_FILE)
        self.user_data_dir = os.path.join(os.getcwd(), "profiles", CURRENT_PROFILE_NAME)
        
        # [QUAN TR·ªåNG] Bi·∫øn ƒë·∫øm t·ªïng s·ªë Comment (ƒë·ªÉ t·∫°o ID COM_xxx)
        self.comment_counter = 0         
        self.current_post_id = ""       
        
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        
        # [C·∫¨P NH·∫¨T] Header theo y√™u c·∫ßu
        self.headers = [
            'comment_id',     # ID t·ª± tƒÉng (COM_001)
            'source_channel', # Ngu·ªìn
            'post_id',        # ID b√†i vi·∫øt g·ªëc
            'timestamp',      # Th·ªùi gian
            'user_id',        # ID ng∆∞·ªùi comment (FB_...)
            'social_user',    # T√™n ng∆∞·ªùi comment
            'original_text',  # N·ªôi dung
            'comment_fb_id'   # ID g·ªëc c·ªßa Facebook
        ]
        
        with open(self.output_path, "w", newline="", encoding="utf-8-sig") as f:
            csv.writer(f).writerow(self.headers)
        print(f"üßπ [INIT] ƒê√£ t·∫°o file s·∫°ch: {OUTPUT_COMMENTS_FILE}")

    # ==========================================================================
    # H√ÄM H·ªñ TR·ª¢
    # ==========================================================================
    def extract_numeric_id(self, base64_id):
        """Gi·∫£i m√£ ID Base64 sang s·ªë (N·∫øu c·∫ßn)"""
        if not base64_id: return "Unknown"
        try:
            if re.match(r'^\d+$', str(base64_id)): return str(base64_id)
            decoded_bytes = base64.b64decode(base64_id)
            decoded_str = decoded_bytes.decode('utf-8')
            match = re.search(r'(\d+)$', decoded_str)
            if match: return match.group(1)
        except: pass
        return base64_id

    def read_posts_from_csv(self):
        """ƒê·ªçc link b√†i vi·∫øt t·ª´ CSV"""
        posts = []
        if not os.path.exists(self.input_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file input: {self.input_path}")
            return posts
        with open(self.input_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('post_link'):
                    posts.append({'post_id': row['post_id'], 'post_link': row['post_link']})
        print(f"üìÇ [READ] ƒê√£ ƒë·ªçc {len(posts)} b√†i vi·∫øt.")
        return posts

    def save_to_csv(self, items):
        """L∆∞u danh s√°ch comment v√†o file"""
        if not items: return
        
        with open(self.output_path, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            for item in items:
                # [C·∫¨P NH·∫¨T] Logic tƒÉng ID
                self.comment_counter += 1
                com_id = f"COM_{self.comment_counter:03d}" 
                
                raw_uid = item.get("author_id", "unknown")
                user_real_id = f"FB_{raw_uid}" if raw_uid != "unknown" else "FB_Unknown"

                writer.writerow([
                    com_id,                 # comment_id
                    'Facebook',             # source_channel
                    self.current_post_id,   # post_id
                    item.get("time"),       # timestamp
                    user_real_id,           # user_id
                    item.get("name"),       # social_user
                    item.get("text"),       # original_text
                    item.get("id")          # comment_fb_id
                ])
                print(f"      + [{self.current_post_id}] {item.get('name')}: {item.get('text')[:30]}...")

    # ==========================================================================
    # H√ÄM B√ìC T√ÅCH D·ªÆ LI·ªÜU
    # ==========================================================================
    def find_text_recursively(self, data, depth=0):
        """T√¨m text ·∫©n s√¢u trong JSON"""
        if depth > 5: return ""
        if isinstance(data, dict):
            if "text" in data and isinstance(data["text"], str) and len(data["text"]) > 0: return data["text"]
            for k, v in data.items():
                if k not in ["__typename", "id"]:
                    res = self.find_text_recursively(v, depth + 1)
                    if res: return res
        elif isinstance(data, list):
            for item in data:
                res = self.find_text_recursively(item, depth + 1)
                if res: return res
        return ""

    def parse_comments_json(self, data, collected_items):
        """Ph√¢n t√≠ch JSON comment"""
        if isinstance(data, dict):
            if data.get("__typename") == "Comment":
                # L·∫•y n·ªôi dung
                body = self.find_text_recursively(data.get("body", {})) or self.find_text_recursively(data)
                
                # L·∫•y t√°c gi·∫£
                author_obj = data.get("author", {})
                author_name = author_obj.get("name", "Unknown")
                author_id = author_obj.get("id", "unknown")

                # L·∫•y ID v√† s·ªë h√≥a n√≥
                raw_comment_id = data.get("id", "")
                numeric_comment_id = self.extract_numeric_id(raw_comment_id)

                # L·∫•y th·ªùi gian
                time_str = ""
                try:
                    ts = data.get("created_time")
                    if ts: time_str = datetime.fromtimestamp(int(ts)).strftime('%Y-%m-%d %H:%M:%S')
                except: pass

                if body:
                    collected_items.append({
                        "id": numeric_comment_id,
                        "author_id": author_id,
                        "name": author_name,
                        "text": body.replace("\n", " "),
                        "time": time_str
                    })
            for val in data.values(): self.parse_comments_json(val, collected_items)
        elif isinstance(data, list):
            for item in data: self.parse_comments_json(item, collected_items)

    # ==========================================================================
    # H√ÄM CH·∫†Y CH√çNH
    # ==========================================================================
    async def run(self):
        posts_to_crawl = self.read_posts_from_csv()
        if not posts_to_crawl: return

        async with async_playwright() as p:
            print(f"üöÄ Profile: {CURRENT_PROFILE_NAME}")
            context = await p.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir, headless=False,
                args=["--disable-notifications"], viewport={"width": 1280, "height": 800}
            )
            page = context.pages[0]

            # --- L·∫ÆNG NGHE M·∫†NG ---
            async def handle_response(response):
                if response.request.resource_type in ["xhr", "fetch"]:
                    try:
                        text = await response.text()
                        if text.startswith("for (;;);"): text = text[9:]
                        if '"Comment"' in text or '"feedback"' in text:
                            try:
                                items = []
                                self.parse_comments_json(json.loads(text), items)
                                if items: self.save_to_csv(items)
                            except: pass
                    except: pass
            page.on("response", handle_response)

            total = len(posts_to_crawl)
            for i, post in enumerate(posts_to_crawl):
                self.current_post_id = post['post_id'] 
                link = post['post_link']
                
                print(f"\n[{i+1}/{total}] üåê {self.current_post_id} | {link}")
                try:
                    await page.goto(link)
                    await page.wait_for_timeout(4000)

                    # 1. Ch·ªânh b·ªô l·ªçc (Most Recent -> All Comments)
                    print("    ‚öôÔ∏è Ch·ªânh b·ªô l·ªçc...")
                    try:
                        filter_btn = page.locator("div[role='button']:has-text('Ph√π h·ª£p nh·∫•t'), div[role='button']:has-text('Most relevant')").first
                        if await filter_btn.is_visible():
                            await filter_btn.click()
                            await page.wait_for_timeout(2000)
                            all_opt = page.locator("div[role='menuitem']:has-text('T·∫•t c·∫£ b√¨nh lu·∫≠n'), div[role='menuitem']:has-text('All comments')").first
                            if await all_opt.is_visible():
                                await all_opt.click()
                                await page.wait_for_timeout(3000)
                            else:
                                newest_opt = page.locator("div[role='menuitem']:has-text('M·ªõi nh·∫•t'), div[role='menuitem']:has-text('Newest')").first
                                if await newest_opt.is_visible(): await newest_opt.click(); await page.wait_for_timeout(3000)
                    except: pass

                    # 2. Cu·ªôn t·∫£i comment
                    print(f"    üîÑ ƒêang cu·ªôn...")
                    last_count = 0
                    retry_count = 0
                    while True:
                        current_count = await page.locator("div[role='article'][aria-label*='luan'], div[role='article'][aria-label*='ment']").count()
                        if current_count == 0: current_count = await page.locator("div[role='article']").count()

                        if current_count == last_count and current_count > 0:
                            retry_count += 1
                            print(f"      ‚ö†Ô∏è Ch∆∞a th·∫•y m·ªõi ({retry_count}/{MAX_RETRIES})...")
                            if retry_count >= MAX_RETRIES:
                                print(f"      üõë D·ª´ng b√†i n√†y. ")
                                break
                        else:
                            if current_count > last_count:
                                print(f"      ‚¨áÔ∏è T·∫£i th√™m {current_count - last_count}...")
                                retry_count = 0
                            last_count = current_count

                        await page.keyboard.press("End")
                        await page.wait_for_timeout(SCROLL_DELAY * 1000)
                        
                        # Click "Xem th√™m" n·∫øu c√≥
                        try:
                            view_more = page.locator("span:text('Xem th√™m b√¨nh lu·∫≠n'), span:text('View more comments')").first
                            if await view_more.is_visible(): await view_more.click(); await page.wait_for_timeout(2000)
                        except: pass
                except Exception as e:
                    print(f"    ‚ö†Ô∏è L·ªói: {e}")

            print(f"\nüéâ HO√ÄN TH√ÄNH! File: {OUTPUT_COMMENTS_FILE}")

if __name__ == "__main__":
    crawler = FacebookCommentCrawler()
    asyncio.run(crawler.run())